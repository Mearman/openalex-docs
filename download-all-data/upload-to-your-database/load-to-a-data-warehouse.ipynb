{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-181717?style=for-the-badge&logo=GitHub&link=https://github.com/Mearman/openalex-docs/tree/develop)](https://github.com/Mearman/openalex-docs/tree/develop)[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-181717?style=for-the-badge&logo=github&link=https://github.com/Mearman/openalex-docs/blob/develop/download-all-data/upload-to-your-database/load-to-a-data-warehouse.ipynb)](https://github.com/Mearman/openalex-docs/blob/develop/download-all-data/upload-to-your-database/load-to-a-data-warehouse.ipynb)[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=for-the-badge&logo=Google%20Colab&link=https://colab.research.google.com/github/Mearman/openalex-docs/blob/develop/download-all-data/upload-to-your-database/load-to-a-data-warehouse.ipynb)](https://colab.research.google.com/github/Mearman/openalex-docs/blob/develop/download-all-data/upload-to-your-database/load-to-a-data-warehouse.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install --upgrade \"git+https://github.com/Mearman/openalex-python-pydantic-v1.git\"\n",
        "%pip install pandasai"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openalex_api import Configuration, ApiClient, AutocompleteApi, AuthorsApi, ConceptsApi, FundersApi, InstitutionsApi, PublishersApi, SourcesApi, WorksApi\n",
        "\n",
        "configuration = Configuration(host=\"https://api.openalex.org\")\n",
        "autocomplete_api = AutocompleteApi(ApiClient(configuration))\n",
        "authors_api = AuthorsApi(ApiClient(configuration))\n",
        "concepts_api = ConceptsApi(ApiClient(configuration))\n",
        "funders_api = FundersApi(ApiClient(configuration))\n",
        "institutions_api = InstitutionsApi(ApiClient(configuration))\n",
        "publishers_api = PublishersApi(ApiClient(configuration))\n",
        "sources_api = SourcesApi(ApiClient(configuration))\n",
        "works_api = WorksApi(ApiClient(configuration))\n",
        "\n",
        "from pandasai import SmartDataframe\n",
        "from pandasai.llm import OpenAI"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title  { run: \"auto\", display-mode: \"form\" }\n",
        "openapi_token = \"\" # @param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load to a data warehouse\n",
        "\n",
        "In many data warehouse and document store applications, you can load the OpenAlex entities as-is and query them directly. We’ll use [BigQuery](https://cloud.google.com/bigquery) as an example here. ([Elasticsearch](https://www.elastic.co/elasticsearch/) docs coming soon). To follow along you’ll need the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install). You’ll also need a Google account that can make BigQuery tables that are, well…  big. Which means it probably won’t be free.\n",
        "\n",
        "We'll show you how to do this in 4 steps:\n",
        "\n",
        "1. Create a BigQuery Project and Dataset to hold your tables\n",
        "2. Create the tables that will hold your entity JSON records\n",
        "3. Copy the data files to the tables you created\n",
        "4. Run some queries on the data you loaded\n",
        "\n",
        "{% hint style=\"info\" %}\n",
        "This guide will have you load each entity to a single text column, then use BigQuery's JSON functions to parse them when you run your queries. This is convenient but inefficient since each object has to be parsed every time you run a query.\n",
        "\n",
        "This project, kindly shared by [@DShvadron](https://twitter.com/DShvadron), takes a more efficient approach: [https://github.com/DrorSh/openalex_to_gbq](https://github.com/DrorSh/openalex_to_gbq)\n",
        "\n",
        "Separating the Entity data into multiple columns takes more work up front but lets you write queries that are faster, simpler, and often [cheaper](https://cloud.google.com/bigquery/pricing#on_demand_pricing).&#x20;\n",
        "{% endhint %}\n",
        "\n",
        "{% hint style=\"info\" %}\n",
        "[Snowflake](https://www.snowflake.com/) users can connect to a ready-to-query data set on the marketplace, helpfully maintained by [Util](https://www.util.co/) - [https://app.snowflake.com/marketplace/listing/GZT0ZOMX4O7](https://app.snowflake.com/marketplace/listing/GZT0ZOMX4O7)\n",
        "{% endhint %}\n",
        "\n",
        "## **Step 1: Create a BigQuery Project and Dataset**\n",
        "\n",
        "In BigQuery, you need a [Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects) and [Dataset](https://cloud.google.com/bigquery/docs/datasets-intro) to hold your tables. We’ll call the project “openalex-demo” and the dataset “openalex”. Follow the linked instructions to create the Project, then create the dataset inside it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "bq mk openalex-demo:openalex"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `Dataset 'openalex-demo:openalex' successfully created`\n",
        "\n",
        "## Step 2: Create tables for each entity type\n",
        "\n",
        "Now, we’ll [create tables](https://cloud.google.com/bigquery/docs/tables) inside the dataset. There will be 5 tables, one for each entity type. Since we’re using JSON, each table will have just one text column named after the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "bq mk --table openalex-demo:openalex.works work:string"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `Table 'openalex-demo:openalex.works' successfully created.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "bq mk --table openalex-demo:openalex.authors author:string"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `Table 'openalex-demo:openalex.authors' successfully created`\n",
        "\n",
        "and so on for `sources`, `institutions`, `concepts,` and `publishers`.\n",
        "\n",
        "## Step 3: Load the data files\n",
        "\n",
        "We’ll load each table’s data from the JSON Lines files we downloaded earlier. For `works`, the files were:&#x20;\n",
        "\n",
        "* openalex-snapshot/data/works/updated_date=2021-12-28/0000_part_00.gz&#x20;\n",
        "* openalex-snapshot/data/works/updated_date=2021-12-28/0001_part_00.gz&#x20;\n",
        "\n",
        "Here’s a command to load one `works` file (don’t run it yet):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "bq load \n",
        "--project_id openalex-demo \n",
        "--source_format=CSV -F '\\t' \n",
        "--schema 'work:string' \n",
        "openalex.works \n",
        "'openalex-snapshot/data/works/updated_date=2021-12-28/0000_part_00.gz'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{% hint style=\"info\" %}\n",
        "See the full documentation for the `bq load` command here: [https://cloud.google.com/bigquery/docs/reference/bq-cli-reference#bq_load](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference#bq_load)\n",
        "{% endhint %}\n",
        "\n",
        "This part of the command may need some explanation:\n",
        "\n",
        "> `--source_format=CSV -F '\\t' --schema 'work:string'`\n",
        "\n",
        "Bigquery is expecting multiple columns with predefined datatypes (a “schema”). We’re tricking it into accepting a single text column (`--schema 'work:string'`) by specifying [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) format (`--source_format=CSV`) with a column delimiter that isn’t present in the file (`-F '\\t')`  (\\t means “tab”).\n",
        "\n",
        "`bq load` can only handle one file at a time, so you must run this command once per file. But remember that the real dataset will have many more files than this example does, so it's impractical to copy, edit, and rerun the command each time. It's easier to handle all the files in a loop, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "for data_file in openalex-snapshot/data/works/*/*.gz;\n",
        "do\n",
        "    bq load --source_format=CSV -F '\\t' \n",
        "        --schema 'work:string' \n",
        "        --project_id openalex-demo \n",
        "        openalex.works $data_file;\n",
        "done"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{% hint style=\"info\" %}\n",
        "This step is slow. _How_ slow depends on your upload speed, but for `Author` and `Work` we're talking hours, not minutes.\n",
        "\n",
        "You can speed this up by using [`parallel`](https://www.gnu.org/software/parallel/) or other tools to run multiple upload commands at once. If you do, watch out for errors caused by hitting [BigQuery quota](https://cloud.google.com/bigquery/docs/troubleshoot-quotas) limits.\n",
        "{% endhint %}\n",
        "\n",
        "Do this once per entity type, substituting each entity name for `work`/`works` as needed. When you’re finished, you’ll have five tables that look like this:\n",
        "\n",
        "![a screenshot of two rows of the works table from the BigQuery console](./<../../.gitbook/assets/Screen Shot 2021-12-29 at 11.57.21 AM.png>.md)\n",
        "\n",
        "## **Step 4: Run your queries!**\n",
        "\n",
        "Now you have the all the OpenAlex data in a place where you can do anything you want with it using [BigQuery JSON functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions) through [bq query](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference#bq_query) or the BigQuery [console](https://console.cloud.google.com/bigquery).&#x20;\n",
        "\n",
        "Here’s a simple one, extracting the OpenAlex ID and OA status for each work:\n",
        "\n",
        "```sql\n",
        "select \n",
        "    json_value(work, '$.id') as work_id, \n",
        "    json_value(work, '$.open_access.is_oa') as is_oa\n",
        "from\n",
        "    `openalex-demo.openalex.works`;\n",
        "```\n",
        "\n",
        "It will give you a list of IDs (this is a truncated sample, the real result will be millions of rows):\n",
        "\n",
        "|                                                                      |       |\n",
        "| -------------------------------------------------------------------- | ----- |\n",
        "| [https://openalex.org/W2741809807](https://openalex.org/W2741809807) | TRUE  |\n",
        "| [https://openalex.org/W1491283979](https://openalex.org/W1491283979) | FALSE |\n",
        "| [https://openalex.org/W1491315632](https://openalex.org/W1491315632) | FALSE |\n",
        "\n",
        "You can run queries like this directly in your shell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "bq query \n",
        "--project_id=openalex-demo \n",
        "--use_legacy_sql=false \n",
        "\"select json_value(work, '$.id') as work_id, json_value(work, '$.open_access.is_oa') as is_oa from openalex.works;\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But even simple queries are hard to read and edit this way. It’s better to write them in a file than directly on the command line. Here’s an example of a slightly more complex query - finding the author with the most open access works of all time:\n",
        "\n",
        "```sql\n",
        "with work_authorships_oa as (\n",
        "   select\n",
        "       json_value(work, '$.id') as work_id,\n",
        "       json_query_array(work, '$.authorships') as authorships,\n",
        "       cast(json_value(work, '$.open_access.is_oa') as BOOL) as is_oa\n",
        "   from `openalex-demo.openalex.works`\n",
        "), flat_authorships as (\n",
        "   select work_id, authorship, is_oa\n",
        "   from work_authorships_oa,\n",
        "   unnest(authorships) as authorship\n",
        ")\n",
        "select \n",
        "    json_value(authorship, '$.author.id') as author_id,\n",
        "    count(distinct work_id) as num_oa_works\n",
        "from flat_authorships\n",
        "where is_oa\n",
        "group by author_id\n",
        "order by num_oa_works desc\n",
        "limit 1;\n",
        "```\n",
        "\n",
        "We get one result:\n",
        "\n",
        "| author_id                       | num_oa_works |\n",
        "| -------------------------------- | -------------- |\n",
        "| https://openalex.org/A2798520857 | 3297           |\n",
        "\n",
        "Checking out [https://api.openalex.org/authors/A2798520857](https://api.openalex.org/authors/A2798520857), we see that this is Ashok Kumar at Manipal University Jaipur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title { run: \"auto\", vertical-output: false }\n",
        "# https://api.openalex.org/authors/A2798520857\n",
        "id=\"A2798520857\" # @param \"A2798520857\" {type: \"string\"}\n",
        "\n",
        "response = authors_api.get_author(\n",
        "\tid=id\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(response).T.rename(columns=lambda x: x[0]).drop(0).set_index('id')\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}